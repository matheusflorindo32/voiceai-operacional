# VoiceAI Operacional ğŸ™ï¸ğŸ¤–  
**Pipeline modular de Voice AI** para transcriÃ§Ã£o (Whisper), processamento com LLM (OpenAI) e sÃ­ntese de voz humanizada (ElevenLabs) â€” com foco em aplicaÃ§Ãµes operacionais, pesquisa e produÃ§Ã£o de conteÃºdo.

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](#)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Ativo-success.svg)](#)
[![Version](https://img.shields.io/badge/Version-1.0.0-informational.svg)](#)

---

## ğŸ”¥ O que este projeto faz
A partir de um Ã¡udio de entrada (MP3/WAV), o sistema executa:

1. **STT â€” Speech-to-Text (Whisper)**  
   Transcreve o Ã¡udio para texto.
2. **LLM â€” Processamento inteligente (OpenAI)**  
   Resume, organiza, responde ou transforma a transcriÃ§Ã£o conforme prompt.
3. **TTS â€” Text-to-Speech (ElevenLabs)**  
   Gera um Ã¡udio final com voz humanizada baseado na resposta do LLM.

---

## ğŸ§  Arquitetura (VisÃ£o Geral)

Ãudio (MP3/WAV)
â†“
[Whisper - STT] â†’ transcricao.txt
â†“
[OpenAI - LLM] â†’ resposta.txt (+ meta.json)
â†“
[ElevenLabs - TTS] â†’ resposta_elevenlabs.mp3


---

## ğŸ“ Estrutura do projeto

voiceai-operacional/

â”œâ”€ core/

â”‚ â”œâ”€ stt.py # transcriÃ§Ã£o (Whisper)

â”‚ â”œâ”€ llm.py # processamento (OpenAI)

â”‚ â””â”€ tts.py # narraÃ§Ã£o (ElevenLabs)

â”œâ”€ data/

â”‚ â”œâ”€ raw/ # Ã¡udios originais (opcional)

â”‚ â”œâ”€ input.wav # Ã¡udio padronizado (16kHz mono)

â”‚ â””â”€ logs/ # logs (se ativado)

â”œâ”€ outputs/

â”‚ â”œâ”€ transcricao.txt

â”‚ â”œâ”€ resposta.txt

â”‚ â”œâ”€ resposta_elevenlabs.mp3

â”‚ â””â”€ meta.json

â”œâ”€ main.py

â”œâ”€ requirements.txt

â”œâ”€ README.md

â”œâ”€ LICENSE

â””â”€ LICENSE.pt.md


---

## âœ… Requisitos
- Python **3.10+**
- Conta/chave de API para:
  - OpenAI (LLM)
  - ElevenLabs (TTS)

> Obs.: Whisper pode rodar local dependendo do setup; no Colab ele roda tranquilo.

---

## âš™ï¸ InstalaÃ§Ã£o (Windows / Linux / Mac)
No terminal dentro da pasta do projeto:

```bash
python -m venv .venv
Windows
.venv\Scripts\activate
Linux/Mac
source .venv/bin/activate

Instalar dependÃªncias:

pip install -r requirements.txt
ğŸ” Configurar variÃ¡veis de ambiente (.env)

Crie um arquivo .env na raiz do projeto (ou copie .env.example):

Exemplo:

OPENAI_API_KEY=coloque_sua_chave_aqui
ELEVENLABS_API_KEY=coloque_sua_chave_aqui
ELEVENLABS_VOICE_ID=coloque_o_voice_id_aqui
MODEL_NAME=gpt-4o-mini

Dica: NUNCA suba seu .env para o GitHub.

ğŸ§ Como usar

Coloque um Ã¡udio em data/

Ideal: data/input.wav (16kHz mono)

Se vocÃª tiver MP3, converta para WAV (16kHz mono)

Converter MP3 â†’ WAV (16kHz mono) com FFmpeg
ffmpeg -y -i "seu_audio.mp3" -ar 16000 -ac 1 "data/input.wav"

Rode o pipeline:

python main.py
ğŸ“¤ SaÃ­das geradas (outputs/)

ApÃ³s rodar, vocÃª terÃ¡:

outputs/transcricao.txt â†’ texto do Ã¡udio

outputs/resposta.txt â†’ resultado processado pelo LLM

outputs/resposta_elevenlabs.mp3 â†’ narraÃ§Ã£o final

outputs/meta.json â†’ metadados do processamento

ğŸ§ª Exemplo de uso real

Use este projeto para:

transformar Ã¡udio de aula em resumo estruturado

gerar roteiro para vÃ­deo

produzir narraÃ§Ã£o para conteÃºdo cientÃ­fico

criar â€œbriefing operacionalâ€ a partir de gravaÃ§Ã£o

ğŸ¥ GIF demonstrativo (como adicionar)

VocÃª pode adicionar um GIF no README para ficar â€œtop portfÃ³lioâ€.

Como fazer (rÃ¡pido):

Grave a tela rodando python main.py (10â€“15s)

Windows: Win + G (Xbox Game Bar) ou ScreenToGif

Converta o vÃ­deo para GIF (ScreenToGif faz isso)

Suba o arquivo em: assets/demo.gif

Adicione no README:

![Demo](assets/demo.gif)
ğŸ›£ï¸ Roadmap (prÃ³ximas versÃµes)

 Modo batch (processar vÃ¡rios Ã¡udios)

 CLI profissional (--input, --output, --prompt)

 Logs estruturados + nÃ­veis (INFO/WARN/ERROR)

 Dockerfile (rodar com 1 comando)

 Testes automatizados

ğŸ“„ LicenÃ§a

Este projeto Ã© licenciado sob a MIT License.
Veja: LICENSE

VersÃ£o em portuguÃªs: LICENSE.pt.md

ğŸ‘¤ Autor

Matheus Florindo de Deus Barboza GonÃ§alves
GitHub: @matheusflorindo32
